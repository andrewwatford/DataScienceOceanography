{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bb491a1-4d39-469c-9c8f-69649b39f8f5",
   "metadata": {},
   "source": [
    "# Machine Learning (ML) \n",
    "\n",
    "## Part II: Deep Learning using Convolutional Neural Networks\n",
    "\n",
    "<img src=\"plankton diagram.png\" width=1000/>\n",
    "\n",
    "This tutorial is part of our Machine Learning day. It demonstrates how to build a deep learning model and prepare an input data pipeline for a real-world ocean science problem -- automatically labelling images of plankton with the correct species label!  \n",
    "\n",
    "### Estimated tutorial time: 1.5 hours\n",
    "\n",
    "## Tutorial goals\n",
    "- Python Skills\n",
    "    - Work with TensorFlow and Keras packages for machine learning\n",
    "    - Load and process large number of images from original source\n",
    "    - Visualise data to make sure it is appropriate for feeding to the deep learning model\n",
    "    - Calculate confusion matrix\n",
    "- ML knowledge\n",
    "    - Learn about supervised learning for image classification \n",
    "    - Learn how to deal with class imbalance in the training data\n",
    "    - Learn how to build a simple Convolutional Neural Network (CNN) model\n",
    "    - Learn how to prepare image data for input into a CNN\n",
    "    \n",
    "## Data\n",
    "\n",
    "- Imaging FlowCytobot (IFCB) is an autonomous submersible flow cytometer developed at the Woods Hole Oceanographic Institute (WHOI). IFCB automatically photographs (through a microscope) plankton from seawater sizes down to around 10 microns. The instrument has been deployed at the Martha's Vineyard Coastal Observatory at a depth of 4m since 2006, providing a continuous time series of plankton images over multiple years. These data are important for studying the structure of ocean ecosystems. There are far too many images for human experts to hand label the species of every plankton, so an automated labelling procedure is necessary. To help train machine learning models for this task, scientists at WHOI produced a dataset of over 3.5 million images that were hand-labelled by experts: https://github.com/hsosik/WHOI-Plankton.\n",
    "\n",
    "- We pre-downloaded the image files from the website using the script __download_plankton.py__ to save time. The first part of this notebook will show how to prepare a dataset from this large quantity of image files, but we also saved a file __processed_dataset.??__ to save time, so you can go ahead and just load this file if you want to try out different deep learning models on the pre-processed data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "## Reading \n",
    "\n",
    "Gonzalez, P., et al., 2019, Automatic plankton quantification using deep features, Journal of Plankton Research (https://doi.org/10.1093/plankt/fbz023).\n",
    "\n",
    "# Let's start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca4cea47-b585-49cb-ad77-dc6f475a3040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import image\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" # uncomment to disable GPU\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053c42a0-74d7-42f7-8308-bdcfc0c44273",
   "metadata": {},
   "source": [
    "## Data, data, data -- so much data!\n",
    "\n",
    "The data consists of thousands of images (.png files) of plankton taken using IFCB. Each plankton image has been classified by an expert. The data is organised into directories with subdirectories for each class. First, let's read through the directories to list all the classes in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417c2211",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/jovyan/shared/WHOI-Plankton/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6f0caa-ebc3-44d7-bceb-7d831a4102fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def png_filenames(year,data_dir):\n",
    "    \n",
    "    dir_name=data_dir + str(year) + '/'\n",
    "\n",
    "    a=!find  \"{dir_name}\" -name \"*.png\"   # use a linux command to find all png files in the folder; it returns a list with paths of *png files\n",
    "    \n",
    "    a=np.array(a)                         # make the list into an numpy array for more convenient operations\n",
    "\n",
    "\n",
    "    exclude_list=np.array(['mix','mix_elongated','detritus','bad']) # define a set of phytoplankton classes to exclude from data\n",
    "\n",
    "    class_name=a.copy();\n",
    "    \n",
    "    for i in range(len(a)):\n",
    "        class_name[i]=a[i].replace(dir_name,'').split('/', 1)[0]    # use the path name to extract the class. The images are organized in folders as /year/class_name/*.png\n",
    "        if np.sum(exclude_list==class_name[i]):\n",
    "            class_name[i]=\"\";\n",
    "\n",
    "    a=a[class_name!=\"\"]                        # remove the empty data from excluded classes\n",
    "    class_name=class_name[class_name!=\"\"]    \n",
    "    \n",
    "    return a, class_name                       # return pathnames and class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31042052-4126-4e80-ad9d-58ca5b082956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract png file paths for a range of years\n",
    "\n",
    "class_name=np.array([])\n",
    "png_names=np.array([])\n",
    "for year in np.arange(2006, 2007): # data available 2006-2014\n",
    "        print(year)\n",
    "        a, b = png_filenames(year,data_dir)\n",
    "        png_names=np.concatenate((png_names, a), axis=0)    #concatenate the data from different years\n",
    "        class_name=np.concatenate((class_name, b), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db62b4f4-e758-4807-b396-362644084959",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, counts = np.unique(class_name, return_counts=True)               # identify the unique classes and associated number of examples \n",
    "classes_sorted=[x for y, x in sorted(zip(counts,classes),reverse=True)]   # sort classes by the number of examples \n",
    "counts_sorted=sorted(counts,reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b5755b-1348-4b2e-9d8a-2fc677f2ed20",
   "metadata": {},
   "source": [
    "Let's take a look at how many images we have for different species':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ba440b-e96f-4026-98cd-7378a77f82b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(classes_sorted[:20], counts_sorted[:20]) #plot top 10 most observed classes\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.ylabel('Number of samples in a given class');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093f545e-2569-42a6-a15f-c33f8c32c6ee",
   "metadata": {},
   "source": [
    "Let's take a look at some of those images, run this cell to see 3 random images from the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eed872-a39b-487f-9e10-9440b4a77ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,3, figsize = (12,8))\n",
    "for i in range(3):\n",
    "    N=np.random.randint(len(png_names))\n",
    "    im=image.imread(png_names[N])\n",
    "    axs[i].imshow(im, cmap = 'gray', vmin = 0, vmax = 1) # the black and white images have pixel values ranging from 0 (black) to 1 (white)\n",
    "    axs[i].set_title(class_name[N]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b9bbce-9a8f-495e-8db3-4711535bba9c",
   "metadata": {},
   "source": [
    "One thing that jumps out straight away is that the images come in very different shapes and sizes. The deep learning model we'll build later requires fixed inputs which aren't too large (to limit the memory requirements during training). To do this, let's take a square cropped image from the center of each image with fixed size, say, 64 pixels. If either dimension of the image is less than 64 we'll pad the image with zeros (i.e. black pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7951db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#crop only the center n by n pixel image from the original data, and fill the image to an n by n size if it is smaller than that\n",
    "def crop_center(im,n):\n",
    "    \n",
    "    nx=im.shape[1]\n",
    "    ny=im.shape[0]\n",
    "\n",
    "    if nx<n:\n",
    "        imm=np.zeros((ny,n))\n",
    "        imm[:ny,:nx]=im\n",
    "        im=imm\n",
    "    \n",
    "    nx=im.shape[1]\n",
    "    ny=im.shape[0]\n",
    "    \n",
    "    if ny<n:\n",
    "        imm=np.zeros((n,nx))\n",
    "        imm[:ny,:nx]=im\n",
    "        im=imm\n",
    "        \n",
    "    nx=im.shape[1]\n",
    "    ny=im.shape[0]\n",
    "    \n",
    "    im=im[ (ny//2-n//2):(ny//2+n//2),(nx//2-n//2):(nx//2+n//2)]\n",
    "\n",
    "    return im"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998c82d5",
   "metadata": {},
   "source": [
    "Let's see how those cropped images look..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a60c59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5,2, figsize = (7,12))\n",
    "for i in range(5):\n",
    "    N=np.random.randint(len(png_names))\n",
    "    im=image.imread(png_names[N])\n",
    "    axs[i][0].imshow(im, cmap = 'gray', vmin = 0, vmax = 1)\n",
    "    im=crop_center(im,64)\n",
    "    axs[i][1].imshow(im, cmap = 'gray', vmin = 0, vmax = 1)\n",
    "    axs[i][0].set_title(class_name[N]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939977a0",
   "metadata": {},
   "source": [
    "We definitely lose some information by taking these small crops from the images, but there should hopefully still be enough distinctive features within the cropped images for our deep learning model to recognise the species!\n",
    "\n",
    "Now that we've explored the data a little, let's go ahead and load in lots of images (extracting square crops from each) into numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2037bcc3-f72f-41c7-9a06-8ff2d7492310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(a,class_name,keep_classes):\n",
    "    \n",
    "    print('Loading the following classes:', keep_classes);\n",
    "    class_name_keep=class_name.copy()\n",
    "    \n",
    "    a_keep=a.copy()\n",
    "\n",
    "    for i in range(len(a)):\n",
    "        if np.sum(keep_classes==class_name_keep[i])==0:\n",
    "            class_name_keep[i]=\"\";\n",
    "\n",
    "    a_keep=a_keep[class_name_keep!=\"\"]\n",
    "    class_name_keep=class_name_keep[class_name_keep!=\"\"]\n",
    "    \n",
    "    \n",
    "    Nsamples=len(a_keep)\n",
    "    IMAGES=np.zeros((Nsamples,64,64))\n",
    "    IM_class=class_name_keep.copy()\n",
    "    \n",
    "    for i in range(Nsamples):\n",
    "        im=image.imread(a[i])\n",
    "        IMAGES[i,:,:]=crop_center(im,64)\n",
    "        \n",
    "        \n",
    "    class_labels=np.arange(len(IM_class))\n",
    "    for i in range(len(keep_classes)):\n",
    "        class_labels[IM_class==keep_classes[i]]=i    \n",
    "    \n",
    "    return IMAGES, IM_class, class_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f03788f",
   "metadata": {},
   "source": [
    "Even with this smaller set of classes, there is a large 'class imbalance' (i.e. there are many more examples of some classes than of others, go back to the histogram we plotted and you'll see there are lots of leptocylindrus images compared to the others!). A CNN trained on this dataset is likely to perform poorly in classifying the least frequently occuring class, as it will have 'seen' fewer examples compared to the other classes. \n",
    "\n",
    "There are a number of ways to deal with class imbalance (https://www.analyticsvidhya.com/blog/2020/07/10-techniques-to-deal-with-class-imbalance-in-machine-learning/), here we'll use 'undersampling'. This means we'll remove samples from the frequently occuring classes until all classes have the same number of samples as the least frequently occuring, thus creating a smaller but balanced data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60259eb2-1194-4fc6-abed-1165ffd44f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_class_samples(png_names,class_name,keep_classes):\n",
    "    \n",
    "    # n_min=np.sum(class_name==keep_classes[-1])\n",
    "    # limit number of images per class to 1000 to run on this server\n",
    "    n_min=1000\n",
    "\n",
    "    png_names_balanced=np.array([])\n",
    "    class_name_balanced=np.array([])\n",
    "\n",
    "    for i in range(len(keep_classes)):\n",
    "        png_names_i=png_names[class_name==keep_classes[i]]\n",
    "        class_name_i=class_name[class_name==keep_classes[i]]\n",
    "\n",
    "        rand_ind=np.random.randint(0,len(png_names_i),n_min);\n",
    "        rand_ind=np.sort(rand_ind)\n",
    "        rand_ind=np.arange(n_min)\n",
    "\n",
    "        png_names_i=png_names_i[rand_ind]\n",
    "        class_name_i=class_name_i[rand_ind]\n",
    "\n",
    "        png_names_balanced=np.concatenate((png_names_balanced, png_names_i), axis=0)\n",
    "        class_name_balanced=np.concatenate((class_name_balanced, class_name_i), axis=0)\n",
    "        \n",
    "    return png_names_balanced,class_name_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842f1367-2167-4a2f-bc08-189fcd0beee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_classes=np.array(classes_sorted[:10])\n",
    "\n",
    "png_names_balanced,class_name_balanced =balance_class_samples(png_names,class_name,keep_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e0c868-1093-4dc7-a98f-028829a79de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "IMAGES, IM_class, class_labels = load_images(png_names_balanced,class_name_balanced,keep_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2865f1d-d63b-443b-a03c-d18db09ed37c",
   "metadata": {},
   "source": [
    "Save the processed data for faster loading in future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0aec03-513b-4a6c-b9cb-34bcd5376285",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('IMAGES.npy', IMAGES)\n",
    "#np.save('IM_class.npy', IM_class)\n",
    "#np.save('class_labels.npy', class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7255e6a-5b95-4285-b815-11cfd097568a",
   "metadata": {},
   "source": [
    "Load processed data for faster loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e454aa-2e3d-49b4-b24d-c51f24c84de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#IMAGES = np.load('IMAGES.npy')\n",
    "#IM_class = np.load('IM_class.npy')\n",
    "#class_labels = np.load('class_labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84e40dd-c2fb-40a1-825c-75c114de7b85",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "Now that we have the desired data loaded in an appropriate format, it's time to design a deep neural network. We will explore Convolutional Neural Networks (CNNS), which are some of the most powerful neural network architectures for analysis of images. You can read up on the basic ideas behind a CNN here: https://levelup.gitconnected.com/a-gentle-introduction-to-convolutional-neural-networks-98923560578f. This type of neural network was pioneered by Yann LeCun and others in the 1990's to recognise hand-written ZIP codes on mail envelopes (their original paper: http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf , it's very long so you don't have to read all of it!). In the decades since, CNNs have exploded in popularity and have been developed for all sorts of state-of-the-art computer vision applications. \n",
    "\n",
    "We won't have time to cover them today, but here are just a few of the other popular neural network architectures used by researchers today: Multilayer Perceptrons (MLPs), Transformer Networks, Generative Adversarial Networks (GANs), Long Short-Term Memory (LSTM), Graph Neural Networks, plus countless more!\n",
    "\n",
    "Here, we'll have a go at developing a simple CNN to predict the species of our plankton images. We'll use the TensorFlow (https://www.tensorflow.org/api_docs) and Keras (https://keras.io/api/) APIs for building and training our neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9ab54f-6eea-47f0-9c01-65e2a5052bbb",
   "metadata": {},
   "source": [
    "A CNN takes as input a tensor of shape (N_rows, N_cols, N_channels) representing an image. Since we're using black and white images, N_channels = 1 for our problem (N_channels = 3 for a colour image, corresponding to RGB). \n",
    "\n",
    "So our input should have shape: (64,64,1), let's reshape our image data and call it X.\n",
    "\n",
    "For the class labels, we'll use 'one-hot' encoding (https://en.wikipedia.org/wiki/One-hot#Machine_learning_and_statistics). So we replace the strings representing our class labels with vectors of length 10 (for the 10 classes) with a 1 in the vector entry corresponding to the right class label and 0 everywhere else. Our CNN will output a similar vector of length 10, with each entry corresponding to the probability the network predicts for the image to belong to each class.\n",
    "\n",
    "Luckily for us, the Keras API can create this one-hot encoding for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bdf82e-ece9-4476-9fd9-24ee38998d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the images for input to CNN\n",
    "X= np.expand_dims(IMAGES, axis=3)\n",
    "# create the one-hot encoding of the training labels\n",
    "Y = keras.utils.to_categorical(class_labels, len(keep_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e432d90-d88d-451c-bcdc-a18f0aedcceb",
   "metadata": {},
   "source": [
    "Now we'll define a simple CNN model with 5 convolutional layers, each followed by a max pooling layer. This sequence of layers allows the network to recognise features from the images independent of where within the image they are (if a plankton is moved 2 microns to the left it's still the same plankton!). These features are aggregated together for the whole image by the GlobalAveragePooling layer before being passed to a fully connected ('Dense') layer to predict the class probabilities from the image features. The weights in the fully connected layer are randomly dropped (Dropout) during training to reduce the risk of overfitting. \n",
    "\n",
    "During training, the network seeks to minimise its loss function. Here, we use 'categorical cross entropy' which takes a smaller value when the predicted probability approaches 100% for the correct class: $$loss = - \\sum_{i=1}^{N}y_i\\text{log}(p_i),$$\n",
    "where $N$ is the number of classes (10 in our case), $y_i$ is the true one-hot encoding value for the i-th class (0 or 1), and $p_i$ is the network's predicted probability for that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26147d7-e1af-4a8a-bcd2-2416e41fc178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a simple CNN model to classify plankton types\n",
    "def create_cnn_model(input_shape, output_shape):\n",
    "\n",
    "    x = keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "    y = keras.layers.Conv2D(8, (3,3), activation='relu', padding='same')(x) # add a convolutional layer with a ReLU activation\n",
    "    y = keras.layers.MaxPooling2D(pool_size=(2,2))(y)     # add a max pooling layer\n",
    "\n",
    "    y = keras.layers.Conv2D(16, (3,3), activation='relu', padding='same')(y)\n",
    "    y = keras.layers.MaxPooling2D(pool_size=(2,2))(y)\n",
    "\n",
    "    y = keras.layers.Conv2D(32, (3,3), activation='relu', padding='same')(y)\n",
    "    y = keras.layers.MaxPooling2D(pool_size=(2,2))(y)\n",
    "\n",
    "    y = keras.layers.Conv2D(64, (3,3), activation='relu', padding='same')(y)\n",
    "    y = keras.layers.MaxPooling2D(pool_size=(2,2))(y)\n",
    "\n",
    "    y = keras.layers.Conv2D(128, (3,3), activation='relu', padding='same')(y)\n",
    "    \n",
    "    y = keras.layers.GlobalAveragePooling2D()(y)\n",
    "\n",
    "    y = keras.layers.Dropout(0.5)(y)\n",
    "\n",
    "#    y = keras.layers.Dense(128, activation='relu')(y)\n",
    "    \n",
    "    y = keras.layers.Dense(output_shape, activation='softmax')(y)     # add final output layer with a softmax activation\n",
    "    \n",
    "    model = keras.models.Model(inputs=x, outputs=y)\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3a025a-f61c-473e-a1eb-2f6a543960a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN = create_cnn_model(input_shape=(64,64,1), output_shape=len(keep_classes))\n",
    "# take a look at how the image tensor changes shape as it passes through the CNN:\n",
    "CNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9040c4e7-e780-4e44-b995-9f95aa9f3457",
   "metadata": {},
   "source": [
    "Our CNN is now ready to train! To prevent overfitting (https://en.wikipedia.org/wiki/Overfitting), we need to split our dataset into a training set and a validation set. The validation set will only be used to monitor the progress during training (the network parameters aren't adjusted in response to the performance on the validation data) and to test the network's accuracy at the end.\n",
    "\n",
    "After splitting the data, we'll then train our model. The number of epochs refers to the number of times the model sees the full training dataset during training. The batch_size refers to the number of examples passed to the model between updates of the network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922d5285-ef1f-40de-a828-e285a2570b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# fraction of the dataset to use for validation\n",
    "val_split=0.2\n",
    "\n",
    "# randomly select the val indices to ensure random mix of classes\n",
    "ind_val= (np.random.rand(X.shape[0])<val_split)\n",
    "\n",
    "X_val=X[ind_val]\n",
    "Y_val=Y[ind_val]\n",
    "\n",
    "class_labels_val=class_labels[ind_val]\n",
    "\n",
    "# remaining data goes in the training dataset\n",
    "X_train=X[~ind_val]\n",
    "Y_train=Y[~ind_val]\n",
    "\n",
    "# train the model, and save the progress to hist (hist will be a dictionary)\n",
    "n_epochs = 20\n",
    "hist = CNN.fit(x = X_train, y = Y_train, epochs=n_epochs, batch_size=512, validation_data=(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edb68fb-ebc4-4a97-b79f-5c164b3d15f9",
   "metadata": {},
   "source": [
    "Let's see how we did!\n",
    "\n",
    "We'll use accuracy as our performance metric. This is the average per-class effectiveness of the class predictor, i.e. $\\frac{TP+TN}{TP+TN+FP+FN}$, where TP = true positive, TN = true negative, FP = false positive, and FN = false negative. So the accuracy would tend to 1 for a perfect predictor. \n",
    "\n",
    "If we randomly guess classes, we should expect the accuracy to be $1/N = 0.1$. So we can use this as a baseline, if we do worse than this then we really messed up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59e39db-b3ae-4a26-8068-ae3e4fe6f7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot how the accuracy on the training and validation datasets evolved during training\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(hist.history['val_accuracy'],label='Validation')\n",
    "plt.plot(hist.history['accuracy'],label='Training')\n",
    "plt.ylim([0,1])\n",
    "plt.hlines(0.1,0,n_epochs-1)\n",
    "plt.annotate('Random Guess',(0, 0.12))\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c299cb7-1e89-4cbc-bb46-6929fc399f3a",
   "metadata": {},
   "source": [
    "**Bonus question to think about at home:** We expect the training accuracy to be higher than the validation accuracy, but for the first few epochs the validation accuracy appears to be higher. Can you explain why?\n",
    "\n",
    "*hint:* It's to do with when in an epoch these accuracies are calculated..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3256b7-4347-4a39-bad0-6c01ca63ac8c",
   "metadata": {},
   "source": [
    "To get a better feel for how the network is doing, let's randomly predict class labels for images from the training data and compare the predicted probabilities to the real labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd1bcaa-0eb6-4e1e-95c0-00ae54ced8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=np.random.randint(0,X_val.shape[0]) # test some random image\n",
    "\n",
    "x=np.expand_dims(X_val[N], axis=0)\n",
    "predictions = CNN.predict(x)\n",
    "\n",
    "class_name_true=keep_classes[Y_val[N,:]==1][0]\n",
    "\n",
    "plt.figure(figsize=(15,3))\n",
    "plt.subplot(1,5,1)\n",
    "plt.imshow(x[0,:,:,0], cmap = 'gray', vmin = 0, vmax = 1)\n",
    "plt.title(class_name_true)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,5,(3,5))\n",
    "plt.bar(keep_classes,predictions[0])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.ylabel('Class probability');\n",
    "ax=plt.gca()\n",
    "ax.grid(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b539324-fdbd-46e3-8309-53e580ea5f01",
   "metadata": {},
   "source": [
    "## Metrics of accuracy: confusion matrix\n",
    "\n",
    "We can look at which classes are being mistaken for one another by the network by plotting a 'confusion matrix'. The rows correspond to the true classes while the columns correspond to the predicted classes. So __confusion_matrix[i,j]__ = the number of examples with true class i, for which the class j was predicted. Entries along the diagonal correspond to correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c110dbbf-62db-469b-a43d-d3b86ca9b07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664bc969-7ab3-4d90-89ce-98b21243a956",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = CNN.predict(X_val)\n",
    "predictions_class=np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1a3c79-8a89-4416-a7e1-acd1f3bd7dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat=confusion_matrix(class_labels_val, predictions_class)\n",
    "\n",
    "conf_mat=conf_mat/ conf_mat.sum(axis=1, keepdims=True); # normalizing by the total number of samples\n",
    "conf_mat=(conf_mat*100).round().astype(int)  # in per cent now, which is good for plotting! \n",
    "# i.e. the % of the samples with true class i for which class j was predicted, so each row adds to 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eae801-da90-4fdc-bd6c-5228f5baaedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=len(keep_classes)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(conf_mat, cmap='Greens',aspect=1)\n",
    "ax=plt.gca()\n",
    "plt.xticks(range(n), keep_classes, rotation='vertical',fontsize=12)\n",
    "plt.yticks(range(n), keep_classes, fontsize=12)\n",
    "plt.xlabel('Predicted Class', fontsize = 18)\n",
    "plt.ylabel('True Class', fontsize = 18)\n",
    "ax.grid(False)\n",
    "plt.title('Confusion Matrix for CNN prediction',fontsize=16)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        plt.text(i-0.25,j+0.1, str(conf_mat[j,i])+'%', fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a581c71-16ca-4e8f-a340-9f41cc2b5b67",
   "metadata": {},
   "source": [
    "As you can see, the network does much better for some classes than others!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f96d0c3-d1c0-4bb5-9ecd-3ec5697cfae6",
   "metadata": {},
   "source": [
    "## In-class exercise: Try adjusting the CNN complexity and seeing how this affects the performance and training\n",
    "\n",
    "Try reducing the number of layers and number of filters in each convolutional layer to see what effect this has on the performance. We'll keep our original model saved as CNN, and create a new model (CNN_simple) then compare the performance of the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7c1392-48da-4c1a-ad94-f43599bef985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_CNN_simple(input_shape, output_shape):\n",
    "    # LEAVE THIS LAYER UNCHANGED:\n",
    "    x = keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "    #############################################\n",
    "    # EDIT THESE LINES:\n",
    "    \n",
    "    # Conv2D syntax: Conv2D(number of filters, filter size, activation function, image padding scheme)\n",
    "    # try reducing the number of filters in the layers\n",
    "    y = keras.layers.Conv2D(8, (3,3), activation='relu', padding='same')(x) # add a convolutional layer with a ReLU activation\n",
    "    y = keras.layers.MaxPooling2D(pool_size=(2,2))(y)     # add a max pooling layer (REDUCES IMAGE HIEGHT AND WIDTH BY FACTOR OF 2)\n",
    "    \n",
    "    # try removing blocks like these next 2 lines:\n",
    "    y = keras.layers.Conv2D(16, (3,3), activation='relu', padding='same')(y)\n",
    "    y = keras.layers.MaxPooling2D(pool_size=(2,2))(y)\n",
    "\n",
    "    y = keras.layers.Conv2D(32, (3,3), activation='relu', padding='same')(y)\n",
    "    y = keras.layers.MaxPooling2D(pool_size=(2,2))(y)\n",
    "\n",
    "    y = keras.layers.Conv2D(64, (3,3), activation='relu', padding='same')(y)\n",
    "    y = keras.layers.MaxPooling2D(pool_size=(2,2))(y) # NOTE YOU CAN ONLY DIVIDE THE HEIGHT/WIDTH BY 2 SO MANY TIME BEFORE HEIGHT=WIDTH=1!\n",
    "\n",
    "    y = keras.layers.Conv2D(128, (3,3), activation='relu', padding='same')(y)\n",
    "    \n",
    "    ############################################\n",
    "    \n",
    "    # LEAVE REST OF THE FUNCTION FROM HERE UNCHANGED\n",
    "    y = keras.layers.GlobalAveragePooling2D()(y)\n",
    "\n",
    "    y = keras.layers.Dropout(0.5)(y)\n",
    "\n",
    "#    y = keras.layers.Dense(128, activation='relu')(y)\n",
    "    \n",
    "    y = keras.layers.Dense(output_shape, activation='softmax')(y)     # add final output layer with a softmax activation\n",
    "    \n",
    "    model = keras.models.Model(inputs=x, outputs=y)\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "CNN_simple = create_CNN_simple(input_shape=(64,64,1), output_shape=len(keep_classes))\n",
    "CNN_simple.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bcc04e-e47f-4ecb-95e5-add0463230ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network\n",
    "# train the model, and save the progress to hist (hist will be a dictionary)\n",
    "n_epochs = 50\n",
    "hist_simple = CNN_simple.fit(x = X_train, y = Y_train, epochs=n_epochs, batch_size=512, validation_data=(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb5ce35-e07a-4214-9d6f-3da54474a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare performance to original CNN model:\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(hist.history['val_accuracy'],label='Validation CNN')\n",
    "plt.plot(hist.history['accuracy'],label='Training CNN')\n",
    "plt.plot(hist_simple.history['val_accuracy'],label='Validation CNN_simple')\n",
    "plt.plot(hist_simple.history['accuracy'],label='Training CNN_simple')\n",
    "plt.ylim([0,1])\n",
    "plt.hlines(0.1,0,n_epochs-1)\n",
    "plt.annotate('Random Guess',(0, 0.12))\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec606f1-e095-4836-a098-b31c75533dec",
   "metadata": {},
   "source": [
    "## Homework exercise: Try adding skip connections -- can you improve on the CNN model's accuracy?\n",
    "\n",
    "He et al. 2015 (https://arxiv.org/abs/1512.03385?context=cs) showed that adding 'skip connections' to a CNN could greatly improve performance. This is known as 'residual learning', and CNNs with this structure are typically known as 'ResNets'.\n",
    "\n",
    "In our simple CNN, the main building block of our network was:\n",
    "```python\n",
    "y = keras.layers.Conv2D(num_filters, (3,3), activation='relu', padding='same')(y)\n",
    "y = keras.layers.MaxPooling2D(pool_size=(2,2))(y)\n",
    "```\n",
    "In ResNet, we'll replace this block with a residual learning block. In this type of block, we will add skip connections to allow some information to skip the convolution layers, before being recombined by adding it to the result of the convolutions.\n",
    "Have a go at building one for yourself using the Keras API (https://keras.io/api/), then stack these blocks together to build a ResNet model (the functions you'll need are keras.layers.Conv2D(), keras.layers.BatchNormalization(), keras.layers.Activation()). Can you improve on the accuracy we gained with the `vanilla' CNN model?\n",
    "\n",
    "Here's a diagram of what one of these blocks looks like:\n",
    "\n",
    "\n",
    "<img align=\"left\" src=\"resnet.png\" width=300/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5a7328-4142-4433-b3f5-16884680051f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_block(y,num_filters):\n",
    "    # your code here\n",
    "    return y\n",
    "\n",
    "def create_ResNet(input_shape, output_shape):\n",
    "    # LEAVE THIS LAYER UNCHANGED:\n",
    "    x = keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    #############################################\n",
    "    # ADD RESIDUAL LEARNING BLOCKS HERE:\n",
    "\n",
    "    ############################################\n",
    "\n",
    "    # LEAVE REST OF THE FUNCTION FROM HERE UNCHANGED\n",
    "    y = keras.layers.GlobalAveragePooling2D()(y)\n",
    "\n",
    "    y = keras.layers.Dropout(0.5)(y)\n",
    "\n",
    "#    y = keras.layers.Dense(128, activation='relu')(y)\n",
    "\n",
    "    y = keras.layers.Dense(output_shape, activation='softmax')(y)     # add final output layer with a softmax activation\n",
    "\n",
    "    model = keras.models.Model(inputs=x, outputs=y)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "ResNet = create_ResNet(input_shape=(64,64,1), output_shape=len(keep_classes))\n",
    "ResNet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c89cbdf-41e3-4e9c-84f3-f7b55d0a6328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network\n",
    "# train the model, and save the progress to hist (hist will be a dictionary)\n",
    "n_epochs = 20\n",
    "hist_resnet = ResNet.fit(x = X_train, y = Y_train, epochs=n_epochs, batch_size=512, validation_data=(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710ccd2a-a8d3-4b18-a274-570b066c344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare performance to original CNN model:\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(hist.history['val_accuracy'],label='Validation CNN')\n",
    "plt.plot(hist.history['accuracy'],label='Training CNN')\n",
    "plt.plot(hist_resnet.history['val_accuracy'],label='Validation ResNet')\n",
    "plt.plot(hist_resnet.history['accuracy'],label='Training ResNet')\n",
    "plt.ylim([0,1])\n",
    "plt.hlines(0.1,0,n_epochs-1)\n",
    "plt.annotate('Random Guess',(0, 0.12))\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-tfGPU]",
   "language": "python",
   "name": "conda-env-.conda-tfGPU-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
