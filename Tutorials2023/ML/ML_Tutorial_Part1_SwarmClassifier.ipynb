{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3caf73c2-3a95-44fa-bdc8-42b6a162ab38",
   "metadata": {},
   "source": [
    "# Machine Learning (ML) \n",
    "\n",
    "## Part I: Unsupervised Classification, Gaussian Mixture Model and K-Means clustering\n",
    "\n",
    "<img src=\"clustering_algos.png\" width=900/>\n",
    "\n",
    "### Estimated Time: 45 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4b68a5-0302-42ce-986e-e6210d83f67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37db6ba2-36e3-4a1e-9c0c-9529bbc9f641",
   "metadata": {},
   "outputs": [],
   "source": [
    "swarm=xr.open_dataset('swarm.nc') #load the saved data from the swarming model simulation\n",
    "#xr.Dataset.close(swarm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a5766b-7391-4ef5-b6e0-b7eec4a11a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_state(X,Y,theta,colors):   \n",
    "\n",
    "    U=np.cos(theta)\n",
    "    V=np.sin(theta)\n",
    "    \n",
    "    plt.scatter(X,Y,30,c=colors,alpha=0.5)\n",
    "    plt.quiver(X,Y,U,V,colors, scale=30)\n",
    "    plt.xlim((-1,1)); plt.ylim((-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7973cb6f-68de-4df0-a972-f027e01f07dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "j=500;\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "plot_state(swarm.x[:,j],swarm.y[:,j],swarm.th[:,j],swarm.colors)\n",
    "plt.title('Clusters or swarms are evident',fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0fed52-fa7c-4216-a1d3-ea58b8670e34",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data analysis, and a common technique for statistical data analysis, used in many fields, including pattern recognition, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning. https://en.wikipedia.org/wiki/Cluster_analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773dcb7f-3906-4dba-a5ca-770f74d9bb26",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Model\n",
    "\n",
    "A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. https://scikit-learn.org/stable/modules/mixture.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92e42a3-1e9f-44e9-ba89-dc568c3b33c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "j=500; #choose some time snapshot\n",
    "\n",
    "features=np.stack((swarm.x[:,j],\n",
    "              swarm.y[:,j],\n",
    "              np.cos(swarm.th[:,j]),\n",
    "              np.sin(swarm.th[:,j])),\n",
    "              axis=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a3c9df-a11b-425b-b783-753549ad6818",
   "metadata": {},
   "outputs": [],
   "source": [
    "K=30 # number of clusters\n",
    "\n",
    "gmm = GaussianMixture(n_components=K)\n",
    "labels_gmm=gmm.fit_predict(features)\n",
    "x_mean,y_mean=gmm.means_[:,0],gmm.means_[:,1]\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plot_state(swarm.x[:,j],swarm.y[:,j],swarm.th[:,j],labels_gmm)\n",
    "plt.title('Gaussian Mixture Model clastering')\n",
    "for k in range(K):\n",
    "    plt.plot(x_mean[k],y_mean[k],'rs',markersize=6)\n",
    "    plt.annotate(str(k), (x_mean[k],y_mean[k]), fontsize=20)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fa20fc-cc67-4ce6-be7d-7a720d4e82c9",
   "metadata": {},
   "source": [
    "## Bayesian information criterion (BIC)\n",
    "\n",
    "In statistics, the Bayesian information criterion (BIC) or Schwarz information criterion (also SIC, SBC, SBIC) is a criterion for model selection among a finite set of models; models with lower BIC are generally preferred. It is based, in part, on the likelihood function and it is closely related to the Akaike information criterion (AIC).\n",
    "\n",
    "When fitting models, it is possible to increase the likelihood by adding parameters, but doing so may result in overfitting. Both BIC and AIC attempt to resolve this problem by introducing a penalty term for the number of parameters in the model. https://en.wikipedia.org/wiki/Bayesian_information_criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ffb03f-0206-4514-bc47-a458718d115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIC=np.array([]);\n",
    "for K in range(1,50):\n",
    "    gmm = GaussianMixture(n_components=K)\n",
    "    gmm.fit_predict(features)\n",
    "    bic=gmm.bic(features)\n",
    "    BIC=np.append(BIC,bic)\n",
    "\n",
    "plt.plot(BIC); \n",
    "plt.xlabel('Number of classes'); \n",
    "plt.ylabel('Bayesian information criterion (BIC)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf2c41d-b686-4079-afe5-384c8700b63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "K=15 #number of classes to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1982fc-4ec6-4634-ba68-0909eadd67ac",
   "metadata": {},
   "source": [
    "## K-means clustering\n",
    "\n",
    "K-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. k-means clustering minimizes within-cluster variances (squared Euclidean distances). https://en.wikipedia.org/wiki/K-means_clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8515f5c-258a-489d-8856-4d484d2c4861",
   "metadata": {},
   "source": [
    "## In class exercise \\#1: K-means clustering\n",
    "\n",
    "- Follow an example of Gaussian Mixture Model above to cluster the data using the k-means algorithm. The sklearn documentation for k-means is here https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "\n",
    "- Make the same plot as above that denotes the clusters with colors and plots the centers of clusters with their cluster numbers. Compare the clustering by GMM with k-means: which is better?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251d5d68-2e3c-4b12-8e97-8f416a11aaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966bfad2-8a8c-4e28-96c2-d90ecad3c86f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c226fb-e3f2-4bb5-b0ce-6989004ee104",
   "metadata": {},
   "outputs": [],
   "source": [
    "K=20\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "ini=True\n",
    "\n",
    "for j in np.arange(400,500,5):\n",
    "        \n",
    "    features=np.stack((swarm.x[:,j],\n",
    "              swarm.y[:,j],\n",
    "              np.cos(swarm.th[:,j]),\n",
    "              np.sin(swarm.th[:,j])),\n",
    "              axis=1);\n",
    "    \n",
    "    if ini: kmeans = KMeans(n_clusters=K).fit(features); ini=False\n",
    "    else:   kmeans = KMeans(n_clusters=K, n_init=1,init=kmeans.cluster_centers_).fit(features)\n",
    "    \n",
    "    x_mean, y_mean =kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1]\n",
    "    \n",
    "    labels=kmeans.labels_\n",
    "\n",
    "\n",
    "    #plotting results\n",
    "    display.display(plt.gcf()); plt.clf()\n",
    "    \n",
    "    plot_state(swarm.x[:,j],swarm.y[:,j],swarm.th[:,j],labels)   \n",
    "    \n",
    "    for k in range(K):\n",
    "        plt.plot(x_mean[k],y_mean[k],'rs',markersize=6)\n",
    "        plt.annotate(str(k), (x_mean[k],y_mean[k]), fontsize=20)\n",
    "        \n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90dc4db-ee5d-46e9-87d2-1732b23b5997",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
