{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd    #dataframe to work with tables\n",
    "import numpy as np     #for general calculations with arrays\n",
    "import pycmap          #API for querrying the data from CMAP\n",
    "import datetime\n",
    "import cdsapi\n",
    "\n",
    "import plotly.express as px   #makes nice interactive plots\n",
    "\n",
    "\n",
    "import cartopy.crs as ccrs         # to plot maps with different projections\n",
    "import cartopy.feature as cfeature # to plot coastlines, land, borders, etc.\n",
    "import matplotlib.pyplot as plt  #makes regular plots\n",
    "\n",
    "from matplotlib.colors import LogNorm  #to make logarithmic colormap axis\n",
    "\n",
    "import xarray as xr  #to work with data\n",
    "\n",
    "from scipy.interpolate import griddata   #interpolate irregularly spaced data onto a grid\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enter your personal API key from CMAP here\n",
    "key = \"0e194d46-b937-4edf-a141-05ca24dd0224\"                     \n",
    "#call the CMAP API using your unique key\n",
    "cmap_client = pycmap.API(token=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the chl data\n",
    "YEAR_MIN, YEAR_MAX = (1970, 2020)\n",
    "LAT_MIN, LAT_MAX = (30, 48)\n",
    "LON_MIN, LON_MAX = (-130, -115)\n",
    "modis=cmap_client.query(\n",
    "         f'''\n",
    "         SELECT [time], AVG(chl) AS chl FROM tblCHL_REP \n",
    "         WHERE \n",
    "         [time] BETWEEN '{YEAR_MIN}' AND '{YEAR_MAX}' AND \n",
    "         lat BETWEEN {LAT_MIN} AND {LAT_MAX} AND \n",
    "         lon BETWEEN {LON_MIN} AND {LON_MAX}\n",
    "         GROUP BY [time] \n",
    "         ORDER BY [time]\n",
    "         '''\n",
    "         )\n",
    "# Add the chl data to a dataframe\n",
    "chl_dat = pd.DataFrame(modis)[:-1]\n",
    "chl_dat.time = [datetime.datetime(int(t[:4]), int(t[5:7]), 1) for t in chl_dat.time]\n",
    "chl_dat = chl_dat.groupby('time').mean('time')\n",
    "chl_dat = chl_dat.reset_index()\n",
    "del modis # free some memory by deleting the no longer needed variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ERA5 data to a dataframe\n",
    "ERA5_Coarse ='./PO/ERA5/ERA5_Coarse.nc'    #loading from local path\n",
    "era = xr.open_dataset(ERA5_Coarse).sel(time=slice(str(YEAR_MIN), str(YEAR_MAX-1)),\n",
    "                                       latitude=slice(str(LAT_MAX), str(LAT_MIN)),\n",
    "                                       longitude=slice(str(360+LON_MIN), str(360+LON_MAX)))   #xarray can open different format data, netcdf is one of them\n",
    "era_avg_dat = era.mean(['longitude', 'latitude']).to_dataframe()\n",
    "era_avg_dat = era_avg_dat.reset_index();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dataframes\n",
    "df = pd.merge(chl_dat, era_avg_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin the data into categorical variables\n",
    "df_processed = pd.DataFrame()\n",
    "n_bins = 5 # seems reasonable?\n",
    "keys_to_drop = ['time', 't2m_F']\n",
    "bin_dict = {}\n",
    "for key in df.keys():\n",
    "    if key not in keys_to_drop:\n",
    "        new_col, bins = pd.cut(df[key], n_bins, labels=False, retbins=True)\n",
    "        bin_dict[key] = bins\n",
    "        df_processed.insert(0, key, new_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what I want to do is to formulate some causal hypothesis for all the variables. As an example, maybe we have edges:\n",
    "$$U \\to (CHL, TP), V \\to (CHL, TP)$$\n",
    "so that $CHL \\perp_d TP | UV$. Then we can test this constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003992797594578227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/98/6vx_1r7n22g6mqrnk3j70t5c0000gn/T/ipykernel_42714/124930367.py:7: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  stat, p = sp.stats.pearsonr(df_processed['tp'][df_processed['uv'] == bin],\n"
     ]
    }
   ],
   "source": [
    "# Group binning of U and V\n",
    "df_processed.insert(0, 'uv', [n_bins*row['u10'] + row['v10'] for idx, row in df_processed.iterrows()])\n",
    "uv_stratified = df_processed.groupby('uv')\n",
    "pvalues_hyp = {}\n",
    "for bin, group in uv_stratified.groups.items():\n",
    "    if len(group) > 1:\n",
    "        stat, p = sp.stats.pearsonr(df_processed['tp'][df_processed['uv'] == bin],\n",
    "                                    df_processed['chl'][df_processed['uv'] == bin])\n",
    "        pvalues_hyp[bin] = p\n",
    "print(min(pvalues_hyp.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that we have a statistically significant correlation between $CHL$ and $TP$, even conditioned on the wind, so we must *reject* our causal hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what I actually want is to incorporate the temporality somehow, and there are general tools for this, e.g.:\n",
    "- https://www.statsmodels.org/stable/tsa.html\n",
    "- https://www.statsmodels.org/stable/contingency_tables.html\n",
    "- https://causal-learn.readthedocs.io/en/latest/\n",
    "- https://fentechsolutions.github.io/CausalDiscoveryToolbox/html/index.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dso_uw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
